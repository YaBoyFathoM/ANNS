<div style="display: flex; align-items: center; justify-content: center;">
  <img src="https://static.wixstatic.com/media/cef1ec_991a7546c9964e3487062bb405395c4b~mv2.png" alt="Logo">
  <p style="margin-left: 20px;">
    <strong>Building a Constitutional Reward Model</strong><br>
    <img src="https://static.wixstatic.com/media/cef1ec_bac6c990ad67420abfe8fd7ca10924bc~mv2.png" alt="Logo">
    <strong>via gamified penetration testing.</strong><br><br>
    RHLF has proven an effective alignment strategy for LLMs, but remains limited by the cost of contract labeling and a scarcity of relevant datasets.<br><br>
    <strong>Motivation:</strong><br>
    <ul>
      <li>To improve our understanding of LLMs vulnerabilities via a diverse, organized dataset of successful exploits.</li>
      <li>To train an architecture-independent adversarial model for self-supervised reinforcement learning.</li>
      <li>To establish a standard metric for LLM behavior evaluation.</li>
    </ul><br>
    <strong>Methodology:</strong><br>
    <ul>
      <li>Provide an infrastructure for the public to post and complete “bounties” by invoking specific behavior in their chosen model</li>
      <li>Evaluate user submissions in three steps:
          <li>Is a chat conversation</li>
          <li>Is the specified model</li>
          <li>Is pass/fail</li>
      </li>
      <li>Clean submission and divide text into human and model, labeling goal prompt as the bounty description.</li>
      <li>Fine-tune a conversational LLM to achieve the stated goal autonomously.</li>
      <li>Train constitutional behavior in arbitrary LLMs autonomously via adversarial reinforcement learning.</li>
    </ul>
  </p>
</div>
